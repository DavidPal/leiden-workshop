\documentclass[largefonts,landscape]{sciposter}

\usepackage{pstricks}
\usepackage[pdftex]{graphicx}
\usepackage{multirow,array}
\usepackage{amsmath,amsthm,amssymb}
\usepackage{algorithmic}
\usepackage{fancybox}
\usepackage{multicol}
\usepackage{natbib}

% \definecolor{mainCol}{rgb}{0.878,0.707,0.769} %% background color
% \definecolor{SectionCol}{rgb}{0.1,0.1,0.5}    %% section heading color
\definecolor{BoxCol}{rgb}{0.66,0.25,0.95}	%% algorithm box color

\newtheorem*{theorem}{Theorem}
\newtheorem*{lemma}{Lemma}
\newtheorem*{definition}{Definition}
\newtheorem*{conjecture}{Conjecture}

\DeclareMathOperator*{\minimize}{minimize}
\DeclareMathOperator*{\Exp}{\mathbf{E}}
\DeclareMathOperator{\Regret}{Regret}
\DeclareMathOperator{\Reward}{Reward}
\DeclareMathOperator{\Risk}{Risk}
\DeclareMathOperator{\polylog}{polylog}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}

\newcommand{\R}{\mathbb{R}}
\newcommand{\indicator}{\mathbf{1}}
\newcommand{\norm}[1]{\left\|#1\right\|}
\newcommand{\KL}[2]{KL\left({#1}\middle\|{#2}\right)}
\newcommand{\grad}{\nabla}
\newcommand{\Breg}{\mathcal{B}}

\begin{document}

\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\renewcommand{\footlogo}{Typeset by pdf\LaTeX}

\title{Parameter-Free and Scale-Free Online Algorithms}
\author{Francesco Orabona\footnotemark[1], D\'avid P\'al\footnotemark[2]}
\institute{%
\footnotemark[1] Stony Brook University, NY\\
\footnotemark[2] Yahoo Research, New York, NY}

\date{November 9, 2016}

\leftlogo[1.2]{uwaterloo-uw}
\rightlogo[1.2]{yahoo-logo}
\conference{Theoretical Foundations for Learning from Easy Data, November 7--11, 2016, Lorentz Center, University of Leiden, Leiden, Netherlands}

\maketitle

\setlength{\parindent}{2em}

\setlength{\columnsep}{6cm}
\begin{multicols}{3}

\section*{Online Learning}

\noindent
Non-empty closed convex \emph{decision set} $K \subseteq V$.

\vspace{1cm}

\setlength{\fboxrule}{5pt}
\setlength{\fboxsep}{10pt}
\begin{center}
Online Linear Optimization
\colorbox[rgb]{0.80,0.55,0.98}{\fbox{
\begin{minipage}{0.8\linewidth}
\begin{algorithmic}
{
\FOR{$t=1,2,3,\dots$}
\STATE Pick $w_t \in K$
\STATE Receive $\ell_t \in V^*$
\STATE Suffer loss $\langle \ell_t, w_t \rangle$
\ENDFOR
}
\end{algorithmic}
\end{minipage}
}}
\end{center}

\vspace{1cm}

$$
\Regret_T(u) = \sum_{t=1}^T \langle \ell_t, w_t \rangle - \sum_{t=1}^T \langle \ell_t, u \rangle \; .
$$

\vspace{1cm}

\setlength{\fboxrule}{5pt}
\setlength{\fboxsep}{10pt}
\begin{center}
Follow The Regularized Leader
\colorbox[rgb]{0.80,0.55,0.98}{\fbox{
\begin{minipage}{0.8\linewidth}
\begin{algorithmic}
{
\STATE Initialize $L_0 \leftarrow 0$
\FOR{$t=1,2,3,\dots$}
\STATE Choose a regularizer $R_t:K \to \R$
\STATE $w_t \leftarrow \argmin_{w \in K} \langle L_{t-1}, w \rangle + R_t(w)$
\STATE Predict $w_t$
\STATE Observe $\ell_t \in V^*$
\STATE $L_t \leftarrow L_{t-1} + \ell_t$
\ENDFOR
}
\end{algorithmic}
\end{minipage}
}}
\end{center}

\vspace{1cm}

$$
\Regret_T(u) \le R_{T+1}(u) + R_1^*(0) + \sum_{t=1}^T \Breg_{R_t^*}(-L_t, -L_{t-1}) - R_t^*(-L_t) + R_{t+1}^*(-L_t)
$$

\vspace{1cm}

\setlength{\fboxrule}{5pt}
\setlength{\fboxsep}{10pt}
\begin{center}
Mirror Descent \\
% \vspace{0.5cm}

\colorbox[rgb]{0.80,0.55,0.98}{\fbox{
\begin{minipage}{0.8\linewidth}
\begin{algorithmic}
{
\STATE Choose a regularizer $R_0:K \to \R$
\STATE $w_1 \leftarrow \argmin_{w \in K} R_0(w)$
\FOR{$t=1,2,3,\dots$}
\STATE Predict $w_t$
\STATE Observe $\ell_t \in V^*$
\STATE Choose a regularizer $R_t:K \to \R$
\STATE $w_{t+1} \leftarrow \argmin_{w \in K} \langle \ell_t, w \rangle + \Breg_{R_t}(w, w_t)$
\ENDFOR
}
\end{algorithmic}
\end{minipage}
}}
\end{center}

\vspace{1cm}

$$
\Regret_T(u) \le \sum_{t=1}^T \langle \ell_t, w_{t} - w_{t+1} \rangle - \Breg_{R_t}(w_{t+1}, w_t) + \Breg_{R_t}(u, w_{t}) - \Breg_{R_t}(u, w_{t+1})
$$


\columnbreak

\section*{Scale-Free Algorithms}

\begin{minipage}{0.8\linewidth}
Non-negative strongly convex $R:K \to \R$ w.r.t. $\norm{\cdot}$.
\begin{center}
\def\arraystretch{2}%  1 is the default, change to whatever you need
\everymath{\displaystyle}
\begin{tabular}{c|c}
FTRL & MD \\ \hline
$R_t(u) = \frac{R(u)}{\sqrt{\sum_{s=1}^{t-1} \norm{\ell_s}_*^2}}$ \hspace{1cm} & \hspace{1cm}  $R_t(u) = \frac{R(u)}{\sqrt{\sum_{s=1}^t \norm{\ell_s}_*^2}}$ \\
\end{tabular}
\end{center}
\end{minipage}

\begin{minipage}{1.0\linewidth}
FTRL:
$$
\Regret_T(u) \le \left(2.75 + R(u)\right) \sqrt{\sum_{t=1}^T \norm{\ell_t}_*^2} + 3.5 \min \left\{ D, \sqrt{T-1} \right\} \max_{t \le T} \norm{\ell_t}_*
$$

MD:
$$
\Regret_T(u) \le \left(1 + \sup_{v \in K} \Breg_{R}(u,v) \right) \sqrt{\sum_{t=1}^T \norm{\ell_t}_*^2}
$$
\end{minipage}

\begin{minipage}{1.0\linewidth}
MD bound is much worse than FTRL bound:

Let $v^* = \argmin_{v \in K} R(v)$.
WLOG assume $R(v^*) = 0$ and $\grad R(v^*) = 0$.

\begin{itemize}
\item $R(u) \le \sup_{v \in K} \Breg_{R}(u,v)$
\item $R(u)$ is finite
\item $\sup_{v \in K} \Breg_{R}(u,v) = + \infty$ for some $R$
\end{itemize}
\end{minipage}


\columnbreak

\section*{Parameter-Free Algorithms}



\section*{Open Problems}

\begin{itemize}
\item Is the Hard Margin algorithm consistent in dimension $d \ge 2$?

\item At what rate can $\gamma$, as a function of $m$, decrease so that the Bucketing Algorithm remains consistent?
We know that when $\gamma \in o(\frac{\log m}{m})$ the algorithm is inconsistent. What happens between for rates between $\frac{1}{\sqrt{m}}$
and $\frac{\log m}{m}$.
\end{itemize}

\end{multicols}

\end{document}
