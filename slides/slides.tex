\documentclass[usenames,dvipsnames]{beamer}

\usefonttheme{serif}
\setbeamertemplate{items}[circle]

\usepackage{pxfonts}
\usepackage{mathpazo}
\usepackage{tcolorbox}
\usepackage{soul}


% \definecolor{red}{rgb}{1,0,0}

\beamertemplatenavigationsymbolsempty

\DeclareMathOperator*{\minimize}{minimize}
\DeclareMathOperator*{\Exp}{\mathbf{E}}
\DeclareMathOperator{\Regret}{Regret}
\DeclareMathOperator{\Risk}{Risk}
\DeclareMathOperator{\polylog}{polylog}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}
\newcommand{\R}{\mathbb{R}}
\newcommand{\indicator}{\mathbf{1}}
\newcommand{\norm}[1]{\left\|#1\right\|}
\newcommand{\KL}[2]{D\left({#1}\middle\|{#2}\right)}

\newcommand{\Cite}[1]{{\tiny \textcolor{Blue}{[#1]}}}

\makeatletter
\newcommand\SoulColor{%
\let\set@color\beamerorig@set@color
\let\reset@color\beamerorig@reset@color}
\makeatother
\setstcolor{red}



\title{Parameter-Free and Scale-Free Online Algorithms}

\date{November 9, 2016}
\author{Francesco Orabona \and D\'avid P\'al}
\institute{Stony Brook University \& Yahoo Research, New York}

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\maketitle
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Online Linear Optimization}

Given a convex set $K \subseteq \R^N$

\vspace{0.3cm}

For $t=1,2,\dots$
\begin{itemize}
\item predict $w_t \in K$
\item receive loss vector $\ell_t \in \R^N$
\item suffer loss $\langle \ell_t, w_t \rangle$
\end{itemize}

\vspace{0.3cm}
$$
\Regret_T(u) = \textcolor{ForestGreen}{\underbrace{\sum_{t=1}^T \langle \ell_t, w_t \rangle}_{\text{algorithm's loss}}} \ - \ \textcolor{red}{\underbrace{\sum_{t=1}^T \langle \ell_t, u \rangle}_{\text{competitor's loss}}}
$$

\vspace{0.3cm}

Examples:
\begin{enumerate}
\item $K = \R^N$
\item $K = \Delta_N = \{ x \in \R^N ~:~ x \ge 0, \norm{x}_1 = 1 \}$
\end{enumerate}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Two Types of Adaptivity}

\begin{enumerate}
\item Adaptivity to competitor $u$ \qquad \textcolor{ForestGreen}{\tiny (parameter-free, quantile bounds, \dots)}
\item Adaptivity to scale of $\ell_1, \ell_2, \dots, \ell_T$ \quad \textcolor{ForestGreen}{\tiny (scale-free, second-order bounds, \dots)}
\end{enumerate}

\vspace{1cm}

\begin{block}{Open Problem (Informal)}
Design efficient \textbf{doubly adaptive} algorithms.
\end{block}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{FTRL Bound}

\begin{theorem}[\textcolor{Blue}{CBL'06, SS'11}]
If $R:K \to \R$ is a non-negative $1$-strongly convex function w.r.t. $\norm{\cdot}$, then FTRL
with regularizer $R$ and learning rate $\eta > 0$ satisfies
$$
\forall u \in K \qquad  \Regret_T(u) \le \frac{R(u)}{\eta} + \eta \sum_{t=1}^T \|\ell_t\|_*^2
$$
\end{theorem}

\textcolor{ForestGreen}{
With learning rate $\eta = \sqrt{R(u)/\sum_{t=1}^T \norm{\ell_t}_*^2}$
$$
\Regret_T(u) \le \sqrt{R(u) \sum_{t=1}^T \norm{\ell_t}_*^2}
$$}

\textcolor{red}{Two cheats}
\begin{enumerate}
\item \textcolor{red}{Bound holds only for \textbf{fixed} $u$}
\item \textcolor{red}{Need to know $\sum_{t=1}^T \norm{\ell_t}_*^2$}
\end{enumerate}

\end{frame}

\end{document}
